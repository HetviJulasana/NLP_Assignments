{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  04 Aishawariya Athawale\n",
    "#  08 Pankti Fadia\n",
    "#  10 Hetvi Julasana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith', '!', 'I', \"'m\", 'going', 'to', 'buy', 'some', 'vegetables', '(', 'cucumber', 'and', 'tomatoes', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = \"\"\"Hi Mr. Smith! I'm going to buy some vegetables (cucumber and tomatoes)\n",
    "from the store. Should I pick up some black-eyed peas as well?\"\"\"\n",
    "\n",
    "print(word_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Mr. Smith!', \"I'm going to buy some vegetables (cucumber and tomatoes)from the store.\", 'Should I pick up some black-eyed peas as well?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "my_text = \"\"\"Hi Mr. Smith!\n",
    "I'm going to buy some vegetables (cucumber and tomatoes)from the store.\n",
    "Should I pick up some black-eyed peas as well?\"\"\"\n",
    "\n",
    "print(sent_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:\n",
      "[('Hi', 'Mr.', 'Smith'), ('Mr.', 'Smith', '!'), ('Smith', '!', 'I'), ('!', 'I', \"'m\"), ('I', \"'m\", 'going'), (\"'m\", 'going', 'to'), ('going', 'to', 'buy'), ('to', 'buy', 'some'), ('buy', 'some', 'vegetables'), ('some', 'vegetables', '('), ('vegetables', '(', 'cucumber'), ('(', 'cucumber', 'and'), ('cucumber', 'and', 'tomatoes'), ('and', 'tomatoes', ')'), ('tomatoes', ')', 'from'), (')', 'from', 'the'), ('from', 'the', 'store'), ('the', 'store', '.'), ('store', '.', 'Should'), ('.', 'Should', 'I'), ('Should', 'I', 'pick'), ('I', 'pick', 'up'), ('pick', 'up', 'some'), ('up', 'some', 'black-eyed'), ('some', 'black-eyed', 'peas'), ('black-eyed', 'peas', 'as'), ('peas', 'as', 'well'), ('as', 'well', '?')] \n",
      "\n",
      "letters:\n",
      "[('H', 'i'), ('i', ' '), (' ', 'M'), ('M', 'r'), ('r', '.'), ('.', ' '), (' ', 'S'), ('S', 'm'), ('m', 'i'), ('i', 't'), ('t', 'h'), ('h', '!'), ('!', '\\n'), ('\\n', 'I'), ('I', \"'\"), (\"'\", 'm'), ('m', ' '), (' ', 'g'), ('g', 'o'), ('o', 'i'), ('i', 'n'), ('n', 'g'), ('g', ' '), (' ', 't'), ('t', 'o'), ('o', ' '), (' ', 'b'), ('b', 'u'), ('u', 'y'), ('y', ' '), (' ', 's'), ('s', 'o'), ('o', 'm'), ('m', 'e'), ('e', ' '), (' ', 'v'), ('v', 'e'), ('e', 'g'), ('g', 'e'), ('e', 't'), ('t', 'a'), ('a', 'b'), ('b', 'l'), ('l', 'e'), ('e', 's'), ('s', ' '), (' ', '('), ('(', 'c'), ('c', 'u'), ('u', 'c'), ('c', 'u'), ('u', 'm'), ('m', 'b'), ('b', 'e'), ('e', 'r'), ('r', ' '), (' ', 'a'), ('a', 'n'), ('n', 'd'), ('d', ' '), (' ', 't'), ('t', 'o'), ('o', 'm'), ('m', 'a'), ('a', 't'), ('t', 'o'), ('o', 'e'), ('e', 's'), ('s', ')'), (')', 'f'), ('f', 'r'), ('r', 'o'), ('o', 'm'), ('m', ' '), (' ', 't'), ('t', 'h'), ('h', 'e'), ('e', ' '), (' ', 's'), ('s', 't'), ('t', 'o'), ('o', 'r'), ('r', 'e'), ('e', '.'), ('.', '\\n'), ('\\n', 'S'), ('S', 'h'), ('h', 'o'), ('o', 'u'), ('u', 'l'), ('l', 'd'), ('d', ' '), (' ', 'I'), ('I', ' '), (' ', 'p'), ('p', 'i'), ('i', 'c'), ('c', 'k'), ('k', ' '), (' ', 'u'), ('u', 'p'), ('p', ' '), (' ', 's'), ('s', 'o'), ('o', 'm'), ('m', 'e'), ('e', ' '), (' ', 'b'), ('b', 'l'), ('l', 'a'), ('a', 'c'), ('c', 'k'), ('k', '-'), ('-', 'e'), ('e', 'y'), ('y', 'e'), ('e', 'd'), ('d', ' '), (' ', 'p'), ('p', 'e'), ('e', 'a'), ('a', 's'), ('s', ' '), (' ', 'a'), ('a', 's'), ('s', ' '), (' ', 'w'), ('w', 'e'), ('e', 'l'), ('l', 'l'), ('l', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "print(\"words:\")\n",
    "\n",
    "my_words = word_tokenize(my_text)\n",
    "threegrams = list(ngrams(my_words,3))\n",
    "print(threegrams,\"\\n\")\n",
    "\n",
    "print(\"letters:\")\n",
    "\n",
    "twograms = list(ngrams(my_text,2))\n",
    "print(twograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith!', \"I'm\", 'going', 'to', 'buy', 'some', 'vegetables', '(cucumber', 'and', 'tomatoes)', 'from', 'the', 'store.', 'Should', 'I', 'pick', 'up', '2', 'kg', 'black-eyed', 'peas', 'as', 'well?'] \n",
      "\n",
      "['mith', 'going', 'to', 'buy', 'some', 'vegetables', 'cucumber', 'and', 'tomatoes', 'from', 'the', 'store', 'hould', 'pick', 'up', 'kg', 'black', 'eyed', 'peas', 'as', 'well'] \n",
      "\n",
      "['2'] \n",
      "\n",
      "['Hi', 'Mr', 'Smith', \"I'm\", 'Should']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I'm going to buy some vegetables (cucumber \"\\\n",
    "\"and tomatoes) from the store.\"\\\n",
    "\" Should I pick up 2 kg black-eyed peas as well?\"\n",
    "\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps = True)\n",
    "print(whitespace_tokenizer.tokenize(my_text),\"\\n\")\n",
    "\n",
    "small_tokenizer = RegexpTokenizer(\"[a-z]['\\w']+\")\n",
    "print(small_tokenizer.tokenize(my_text),\"\\n\")\n",
    "\n",
    "num_tokenizer = RegexpTokenizer(\"[0-9]\")\n",
    "print(num_tokenizer.tokenize(my_text),\"\\n\")\n",
    "\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w']+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation using Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi mr smith im going to buy some vegetables cucumber and tomatoes from the store should i pick up 2 kg blackeyed peas as well\n",
      "HI MR SMITH IM GOING TO BUY SOME VEGETABLES CUCUMBER AND TOMATOES FROM THE STORE SHOULD I PICK UP 2 KG BLACKEYED PEAS AS WELL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text = re.sub('[%s]'%re.escape(string.punctuation),'',my_text)\n",
    "#clean_text\n",
    "print(clean_text.lower())\n",
    "print(clean_text.upper(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing a digit from sentence using Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi mr smith im going to buy some vegetables cucumber and tomatoes from the store should i pick up  kg blackeyed peas as well \n",
      "\n",
      "hi mr smith im going to buy some vegetables cucumber an tomatoes from the store shoul i pick up 2 kg blackeye peas as well \n",
      "\n",
      "hi mr smith im going to buy some vegetables cucumber and tomatoes from the store should i pick up  kg blackeyed peas as well \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text1 = re.sub('\\w*\\d\\w*','',clean_text)\n",
    "print(clean_text1.lower(),\"\\n\")\n",
    "\n",
    "clean_text2 = re.sub('d+','',clean_text)\n",
    "print(clean_text2.lower(),\"\\n\")\n",
    "\n",
    "clean_text3 = re.sub('\\d','',clean_text)\n",
    "print(clean_text3.lower(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using regex for pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(57, 60), match='fly'>\n",
      "['fly', 'fly']\n",
      "<re.Match object; span=(25, 29), match='bird'>\n",
      "<re.Match object; span=(0, 1), match='#'>\n",
      "<re.Match object; span=(5, 11), match='played'>\n",
      "['The', 'train', 'in city']\n",
      "['The', 'train in city']\n",
      "(13, 17)\n",
      "The train in city\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "my_text=\"\"\"#This book was about a 8 bird who didn't yet know how to fly.The bird has to decide if it will try to fly, \n",
    "but it was not sure if it wants to. The bird thought, \"If I never forever endeavor\" then I won't ever learn.\n",
    "Do you like the story? there are 5 more stories \"\"\"\n",
    "\n",
    "\n",
    "find=re.search(\"fly\",my_text)\n",
    "print(find)\n",
    "\n",
    "find_all=re.findall(\"fly\",my_text)# Return the list of all the matches of string. \n",
    "print(find_all)\n",
    "\n",
    "\n",
    "pattern = re.compile(\"bird\") #stores the regular expression pattern in the cache memory for faster searches.\n",
    "result = pattern.search(my_text)\n",
    "print(result)\n",
    "\n",
    "pattern1=re.search(\"#\",my_text)\n",
    "print(pattern1)\n",
    "\n",
    "text=\"They played football. and walked to the ground.\"\n",
    "s=re.search(\"(play|walk)ed\",text)\n",
    "print(s)\n",
    "\n",
    "txt = \"The,train,in city\"\n",
    "pattern2 = re.split(\",\", txt)  #Split the string only at the [pattern] occurrence.\n",
    "print(pattern2) \n",
    "\n",
    "txt = \"The train in city\"\n",
    "pattern3 = re.split(\"\\s\", txt,1)  ##Split the string at the first white-space character.\n",
    "print(pattern3) \n",
    "\n",
    "\n",
    "pattern4 = re.search(r\"\\bc\\w+\", txt)#Search for an \"c\" character in the beginning of a word, and print its position:\n",
    "print(pattern4.span())\n",
    "\n",
    "\n",
    "pattern5 = re.search(\"city\", txt)\n",
    "print(pattern5.string)#The string property returns the search string\n",
    "\n",
    "\n",
    "txt = \"The train in city\"\n",
    "pattern6 = re.search(r\"\\bt\\w+\", txt)\n",
    "print(pattern6.group())#returns the part of the string where there was a match\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using lambdas & map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm going to buy  cans of milk\", \"I'm going to buy  of beans\"]\n",
      "[\"I'm going to buy 5 cans of milk\", \"I'm going to buy  of beans\"]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"I'm going to buy 5 cans of milk\"\n",
    "text2 = \"I'm going to buy 10cans of beans\"\n",
    "\n",
    "texts = [text1,text2]\n",
    "remove_num = lambda x:re.sub('\\d\\w*','',x)\n",
    "text = list(map(remove_num,texts))\n",
    "print(text)\n",
    "\n",
    "remove_num1 = lambda x:re.sub('\\d\\w+','',x)\n",
    "text_1 = list(map(remove_num1,texts))\n",
    "print(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s', 'once', 'does', 'being', 'and', 'further', 'hers', 'more', 'aren', 'wouldn', \"you've\", 'mightn', \"shouldn't\", 'for', \"haven't\", 'ours', 'where', 'him', 'if', \"don't\", \"she's\", \"you'd\", 'such', 'isn', 'just', 'was', 'after', 'at', 'with', 'what', \"hadn't\", \"shan't\", \"it's\", \"needn't\", 'before', 'most', 'because', 'some', 'as', 'under', 'can', 'doing', 'this', 'the', 'to', 'here', 'or', 'do', 'then', 'had', 'but', 'through', 'now', 'themselves', 'above', 'that', 'into', 'out', 'am', 'y', 'are', 'below', 'each', 'didn', \"doesn't\", 't', \"mustn't\", 'don', 'itself', 'own', 'herself', 'very', 'whom', 'll', 'of', 'why', 'has', \"you're\", 'who', 'during', \"didn't\", 'yours', 'over', 'my', 'she', 'them', 'those', 'by', 'both', 've', 'so', 'these', 'which', 'mustn', 'other', 'doesn', \"you'll\", 'off', 'myself', 'in', 'your', \"mightn't\", 'himself', 'o', \"weren't\", 'nor', 'between', 're', 'shouldn', 'too', 'they', 'about', 'needn', 'only', 'down', 'his', 'have', 'same', \"wasn't\", 'wasn', 'yourself', 'up', 'did', 'its', 'an', 'there', \"isn't\", 'be', 'against', 'on', 'her', 'than', 'when', 'it', 'again', 'i', 'any', 'will', 'couldn', 'while', 'ma', 'weren', 'd', 'hadn', \"hasn't\", 'were', \"wouldn't\", 'is', 'not', 'hasn', 'shan', 'ain', 'our', 'ourselves', 'won', 'from', 'theirs', 'should', \"won't\", 'you', 'few', \"that'll\", \"aren't\", \"should've\", 'm', 'until', \"couldn't\", 'their', 'yourselves', 'how', 'all', 'haven', 'he', 'me', 'having', 'no', 'we', 'been', 'a'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumber</th>\n",
       "      <th>eyed</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>peas</th>\n",
       "      <th>pick</th>\n",
       "      <th>smith</th>\n",
       "      <th>store</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>vegetables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  buy  cucumber  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
       "0      1    1         1     1      1   1   1     1     1      1      1   \n",
       "\n",
       "   tomatoes  vegetables  \n",
       "0         1           1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text = [\"Hi Mr. Smith! I'm going to buy some vegetables (cucumber and tomatoes) from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "cv = CountVectorizer(stop_words= 'english') # removes stop words\n",
    "#cv = CountVectorizer() \n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns = cv.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
